Training a baseline transformer ASR system...
2020-12-15 10:15:00,655 - /root/multiASR/egs/micArray/s5/../../../src/train.py[line:126] - INFO: 
Model info:
Model(
  (splayer): SPLayer(
    (CConv): ComplexConv(
      (conv_re): Conv2d(16, 16, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))
      (conv_im): Conv2d(16, 16, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))
    )
    (LastCConv): ComplexConv(
      (conv_re): Conv2d(16, 1, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))
      (conv_im): Conv2d(16, 1, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))
    )
    (LastConv): Conv2d(2, 1, kernel_size=(1, 1), stride=(1, 1))
  )
  (encoder): Transformer(
    (sub): Conv2dSubsampleV2(
      (conv): Sequential(
        (subsample/conv0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 1))
        (subsample/relu0): ReLU()
        (subsample/conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 1))
        (subsample/relu1): ReLU()
      )
      (affine): Linear(in_features=8096, out_features=512, bias=True)
    )
    (pe): PositionalEncoding()
    (dropout): Dropout(p=0.1, inplace=False)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (decoder): TransformerDecoder(
    (emb): Embedding(5060, 512)
    (pe): PositionalEncoding()
    (dropout): Dropout(p=0.1, inplace=False)
    (transformer_block): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (output_affine): Linear(in_features=512, out_features=5060, bias=True)
  )
)
2020-12-15 10:15:00,656 - /root/multiASR/egs/micArray/s5/../../../src/train.py[line:137] - INFO: Let's use 6 GPUs!
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
2020-12-15 10:15:31,188 - /root/multiASR/egs/micArray/s5/../../../src/train.py[line:148] - INFO: Start training...
2020-12-15 10:15:31,188 - /root/multiASR/src/trainer.py[line:165] - INFO: Training
/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.
/root/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2020-12-15 10:20:45,447 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 5 | Iter 40:
per_token_loss: 8.5499039 | avg_token_loss: 8.5643826 | learning_rate: 0.0000001
sequence_per_sec: 24.3348657
2020-12-15 10:25:03,967 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 10 | Iter 80:
per_token_loss: 8.5556831 | avg_token_loss: 8.5623617 | learning_rate: 0.0000002
sequence_per_sec: 27.4634653
2020-12-15 10:29:19,786 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 15 | Iter 120:
per_token_loss: 8.5677004 | avg_token_loss: 8.5589027 | learning_rate: 0.0000003
sequence_per_sec: 27.3194944
2020-12-15 10:33:38,406 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 20 | Iter 160:
per_token_loss: 8.5394135 | avg_token_loss: 8.5541162 | learning_rate: 0.0000004
sequence_per_sec: 27.8788877
2020-12-15 10:37:56,748 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 25 | Iter 200:
per_token_loss: 8.5122576 | avg_token_loss: 8.5464201 | learning_rate: 0.0000005
sequence_per_sec: 28.3055925
2020-12-15 10:42:14,710 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 30 | Iter 240:
per_token_loss: 8.4597826 | avg_token_loss: 8.5385218 | learning_rate: 0.0000006
sequence_per_sec: 28.2643982
2020-12-15 10:46:38,150 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 35 | Iter 280:
per_token_loss: 8.4526806 | avg_token_loss: 8.5287199 | learning_rate: 0.0000007
sequence_per_sec: 28.4767927
2020-12-15 10:51:06,271 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 40 | Iter 320:
per_token_loss: 8.4533949 | avg_token_loss: 8.5183935 | learning_rate: 0.0000009
sequence_per_sec: 28.5483181
2020-12-15 10:55:33,924 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 45 | Iter 360:
per_token_loss: 8.4491901 | avg_token_loss: 8.5068007 | learning_rate: 0.0000010
sequence_per_sec: 28.6693880
2020-12-15 11:00:01,342 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 50 | Iter 400:
per_token_loss: 8.3743143 | avg_token_loss: 8.4947233 | learning_rate: 0.0000011
sequence_per_sec: 28.6092239
2020-12-15 11:04:24,049 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 55 | Iter 440:
per_token_loss: 8.3042879 | avg_token_loss: 8.4825640 | learning_rate: 0.0000012
sequence_per_sec: 28.6713778
2020-12-15 11:08:55,113 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 60 | Iter 480:
per_token_loss: 8.3040619 | avg_token_loss: 8.4693899 | learning_rate: 0.0000013
sequence_per_sec: 28.6408705
2020-12-15 11:13:20,046 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 65 | Iter 520:
per_token_loss: 8.2953739 | avg_token_loss: 8.4554205 | learning_rate: 0.0000014
sequence_per_sec: 28.6051015
2020-12-15 11:17:45,114 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 70 | Iter 560:
per_token_loss: 8.2297354 | avg_token_loss: 8.4398546 | learning_rate: 0.0000015
sequence_per_sec: 28.6473502
2020-12-15 11:22:09,941 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 75 | Iter 600:
per_token_loss: 8.0774918 | avg_token_loss: 8.4230547 | learning_rate: 0.0000016
sequence_per_sec: 28.7486960
2020-12-15 11:26:36,242 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 80 | Iter 640:
per_token_loss: 8.1369572 | avg_token_loss: 8.4050035 | learning_rate: 0.0000017
sequence_per_sec: 28.8613213
2020-12-15 11:31:01,791 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 85 | Iter 680:
per_token_loss: 8.0932970 | avg_token_loss: 8.3879309 | learning_rate: 0.0000018
sequence_per_sec: 28.8488705
2020-12-15 11:35:29,681 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 90 | Iter 720:
per_token_loss: 8.0857182 | avg_token_loss: 8.3706322 | learning_rate: 0.0000019
sequence_per_sec: 28.7875996
2020-12-15 11:39:57,325 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 95 | Iter 760:
per_token_loss: 7.9882908 | avg_token_loss: 8.3539324 | learning_rate: 0.0000021
sequence_per_sec: 28.7282301
2020-12-15 11:44:22,861 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 100 | Iter 800:
per_token_loss: 8.0560179 | avg_token_loss: 8.3351927 | learning_rate: 0.0000022
sequence_per_sec: 28.8368911
2020-12-15 11:48:49,672 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 105 | Iter 840:
per_token_loss: 8.0047779 | avg_token_loss: 8.3191328 | learning_rate: 0.0000023
sequence_per_sec: 28.7604676
2020-12-15 11:53:21,424 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 110 | Iter 880:
per_token_loss: 7.9885077 | avg_token_loss: 8.3037796 | learning_rate: 0.0000024
sequence_per_sec: 28.6580595
2020-12-15 11:57:52,397 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 115 | Iter 920:
per_token_loss: 7.7727089 | avg_token_loss: 8.2882261 | learning_rate: 0.0000025
sequence_per_sec: 28.6124411
2020-12-15 12:02:22,181 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 120 | Iter 960:
per_token_loss: 7.9605870 | avg_token_loss: 8.2713709 | learning_rate: 0.0000026
sequence_per_sec: 28.6465593
2020-12-15 12:06:57,020 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 125 | Iter 1000:
per_token_loss: 7.9685221 | avg_token_loss: 8.2568970 | learning_rate: 0.0000027
sequence_per_sec: 28.5620521
2020-12-15 12:11:28,087 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 130 | Iter 1040:
per_token_loss: 7.8627853 | avg_token_loss: 8.2417927 | learning_rate: 0.0000028
sequence_per_sec: 28.5496444
2020-12-15 12:15:59,589 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 135 | Iter 1080:
per_token_loss: 7.8231392 | avg_token_loss: 8.2267570 | learning_rate: 0.0000029
sequence_per_sec: 28.5804070
2020-12-15 12:20:33,962 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 140 | Iter 1120:
per_token_loss: 7.7931623 | avg_token_loss: 8.2122030 | learning_rate: 0.0000030
sequence_per_sec: 28.5940762
2020-12-15 12:25:06,592 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 145 | Iter 1160:
per_token_loss: 7.8249941 | avg_token_loss: 8.1990261 | learning_rate: 0.0000031
sequence_per_sec: 28.5421500
2020-12-15 12:29:36,374 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 150 | Iter 1200:
per_token_loss: 7.8038716 | avg_token_loss: 8.1853657 | learning_rate: 0.0000033
sequence_per_sec: 28.5732295
2020-12-15 12:34:08,337 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 155 | Iter 1240:
per_token_loss: 7.8195653 | avg_token_loss: 8.1729641 | learning_rate: 0.0000034
sequence_per_sec: 28.5363300
2020-12-15 12:38:40,338 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 160 | Iter 1280:
per_token_loss: 7.7914300 | avg_token_loss: 8.1608973 | learning_rate: 0.0000035
sequence_per_sec: 28.4995345
2020-12-15 12:43:10,024 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 165 | Iter 1320:
per_token_loss: 7.8702836 | avg_token_loss: 8.1490040 | learning_rate: 0.0000036
sequence_per_sec: 28.4805808
2020-12-15 12:47:45,081 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 170 | Iter 1360:
per_token_loss: 7.7577233 | avg_token_loss: 8.1366386 | learning_rate: 0.0000037
sequence_per_sec: 28.5110327
2020-12-15 12:52:18,258 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 175 | Iter 1400:
per_token_loss: 7.7458448 | avg_token_loss: 8.1252861 | learning_rate: 0.0000038
sequence_per_sec: 28.5058753
2020-12-15 12:56:48,282 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 180 | Iter 1440:
per_token_loss: 7.8021669 | avg_token_loss: 8.1147289 | learning_rate: 0.0000039
sequence_per_sec: 28.4675566
2020-12-15 13:01:22,084 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 185 | Iter 1480:
per_token_loss: 7.7345748 | avg_token_loss: 8.1036091 | learning_rate: 0.0000040
sequence_per_sec: 28.4692713
2020-12-15 13:05:55,476 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 190 | Iter 1520:
per_token_loss: 7.4783082 | avg_token_loss: 8.0932484 | learning_rate: 0.0000041
sequence_per_sec: 28.4521678
2020-12-15 13:10:30,512 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 195 | Iter 1560:
per_token_loss: 7.6481466 | avg_token_loss: 8.0833712 | learning_rate: 0.0000042
sequence_per_sec: 28.4045939
Traceback (most recent call last):
  File "/root/multiASR/egs/micArray/s5/../../../src/train.py", line 149, in <module>
    trainer.train()
  File "/root/multiASR/src/trainer.py", line 166, in train
    tr_loss = self.iter_one_epoch()
  File "/root/multiASR/src/trainer.py", line 226, in iter_one_epoch
    data = next(loader_iter)
  File "/root/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 345, in __next__
    data = self._next_data()
  File "/root/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 841, in _next_data
    idx, data = self._get_data()
  File "/root/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 808, in _get_data
    success, data = self._try_get_data()
  File "/root/.local/lib/python3.7/site-packages/torch/utils/data/dataloader.py", line 761, in _try_get_data
    data = self._data_queue.get(timeout=timeout)
  File "/root/anaconda3/lib/python3.7/multiprocessing/queues.py", line 113, in get
    return _ForkingPickler.loads(res)
  File "/root/.local/lib/python3.7/site-packages/torch/multiprocessing/reductions.py", line 294, in rebuild_storage_fd
    fd = df.detach()
  File "/root/anaconda3/lib/python3.7/multiprocessing/resource_sharer.py", line 57, in detach
    with _resource_sharer.get_connection(self._id) as conn:
  File "/root/anaconda3/lib/python3.7/multiprocessing/resource_sharer.py", line 87, in get_connection
    c = Client(address, authkey=process.current_process().authkey)
  File "/root/anaconda3/lib/python3.7/multiprocessing/connection.py", line 498, in Client
    answer_challenge(c, authkey)
  File "/root/anaconda3/lib/python3.7/multiprocessing/connection.py", line 741, in answer_challenge
    message = connection.recv_bytes(256)         # reject large message
  File "/root/anaconda3/lib/python3.7/multiprocessing/connection.py", line 216, in recv_bytes
    buf = self._recv_bytes(maxlength)
  File "/root/anaconda3/lib/python3.7/multiprocessing/connection.py", line 407, in _recv_bytes
    buf = self._recv(4)
  File "/root/anaconda3/lib/python3.7/multiprocessing/connection.py", line 383, in _recv
    raise EOFError
EOFError
Error in atexit._run_exitfuncs:
Traceback (most recent call last):
  File "/root/anaconda3/lib/python3.7/multiprocessing/popen_fork.py", line 28, in poll
    pid, sts = os.waitpid(self.pid, flag)
  File "/root/.local/lib/python3.7/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 892066) is killed by signal: Killed. 
Training a baseline transformer ASR system...
2020-12-15 15:44:16,756 - /root/multiASR/egs/micArray/s5/../../../src/train.py[line:126] - INFO: 
Model info:
Model(
  (splayer): SPLayer(
    (CConv): ComplexConv(
      (conv_re): Conv2d(16, 16, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))
      (conv_im): Conv2d(16, 16, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))
    )
    (LastCConv): ComplexConv(
      (conv_re): Conv2d(16, 1, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))
      (conv_im): Conv2d(16, 1, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))
    )
    (LastConv): Conv2d(2, 1, kernel_size=(1, 1), stride=(1, 1))
  )
  (encoder): Transformer(
    (sub): Conv2dSubsampleV2(
      (conv): Sequential(
        (subsample/conv0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 1))
        (subsample/relu0): ReLU()
        (subsample/conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 1))
        (subsample/relu1): ReLU()
      )
      (affine): Linear(in_features=8096, out_features=512, bias=True)
    )
    (pe): PositionalEncoding()
    (dropout): Dropout(p=0.1, inplace=False)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (decoder): TransformerDecoder(
    (emb): Embedding(5060, 512)
    (pe): PositionalEncoding()
    (dropout): Dropout(p=0.1, inplace=False)
    (transformer_block): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (output_affine): Linear(in_features=512, out_features=5060, bias=True)
  )
)
2020-12-15 15:44:16,756 - /root/multiASR/egs/micArray/s5/../../../src/train.py[line:137] - INFO: Let's use 6 GPUs!
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
2020-12-15 15:44:41,306 - /root/multiASR/egs/micArray/s5/../../../src/train.py[line:148] - INFO: Start training...
2020-12-15 15:44:41,307 - /root/multiASR/src/trainer.py[line:165] - INFO: Training
/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.
/root/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2020-12-15 15:49:39,905 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 5 | Iter 40:
per_token_loss: 8.5853739 | avg_token_loss: 8.5884142 | learning_rate: 0.0000001
sequence_per_sec: 26.3167561
2020-12-15 15:53:53,990 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 10 | Iter 80:
per_token_loss: 8.5894651 | avg_token_loss: 8.5871744 | learning_rate: 0.0000002
sequence_per_sec: 27.7235716
2020-12-15 15:58:06,091 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 15 | Iter 120:
per_token_loss: 8.5519934 | avg_token_loss: 8.5843897 | learning_rate: 0.0000003
sequence_per_sec: 28.6646278
2020-12-15 16:02:18,485 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 20 | Iter 160:
per_token_loss: 8.5801573 | avg_token_loss: 8.5802574 | learning_rate: 0.0000004
sequence_per_sec: 29.2562213
2020-12-15 16:06:32,692 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 25 | Iter 200:
per_token_loss: 8.5524950 | avg_token_loss: 8.5728731 | learning_rate: 0.0000005
sequence_per_sec: 29.8578522
2020-12-15 16:10:44,343 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 30 | Iter 240:
per_token_loss: 8.5354185 | avg_token_loss: 8.5644979 | learning_rate: 0.0000006
sequence_per_sec: 30.1639465
2020-12-15 16:14:56,465 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 35 | Iter 280:
per_token_loss: 8.5117426 | avg_token_loss: 8.5557051 | learning_rate: 0.0000007
sequence_per_sec: 30.2716627
2020-12-15 16:19:09,288 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 40 | Iter 320:
per_token_loss: 8.4997807 | avg_token_loss: 8.5456514 | learning_rate: 0.0000009
sequence_per_sec: 30.2930367
2020-12-15 16:23:22,933 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 45 | Iter 360:
per_token_loss: 8.4322653 | avg_token_loss: 8.5351744 | learning_rate: 0.0000010
sequence_per_sec: 30.0563749
2020-12-15 16:27:38,896 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 50 | Iter 400:
per_token_loss: 8.3177614 | avg_token_loss: 8.5222092 | learning_rate: 0.0000011
sequence_per_sec: 29.9147313
2020-12-15 16:32:03,410 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 55 | Iter 440:
per_token_loss: 8.3340111 | avg_token_loss: 8.5069313 | learning_rate: 0.0000012
sequence_per_sec: 29.9543833
2020-12-15 16:36:26,508 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 60 | Iter 480:
per_token_loss: 8.3257484 | avg_token_loss: 8.4919004 | learning_rate: 0.0000013
sequence_per_sec: 29.8625881
2020-12-15 16:40:49,253 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 65 | Iter 520:
per_token_loss: 8.3091335 | avg_token_loss: 8.4757605 | learning_rate: 0.0000014
sequence_per_sec: 29.8436060
2020-12-15 16:45:12,396 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 70 | Iter 560:
per_token_loss: 8.1929426 | avg_token_loss: 8.4583216 | learning_rate: 0.0000015
sequence_per_sec: 29.8652613
2020-12-15 16:49:35,765 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 75 | Iter 600:
per_token_loss: 8.1638908 | avg_token_loss: 8.4408369 | learning_rate: 0.0000016
sequence_per_sec: 29.8608051
2020-12-15 16:53:59,252 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 80 | Iter 640:
per_token_loss: 7.9934483 | avg_token_loss: 8.4228458 | learning_rate: 0.0000017
sequence_per_sec: 29.8473514
2020-12-15 16:58:24,854 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 85 | Iter 680:
per_token_loss: 8.0071068 | avg_token_loss: 8.4038677 | learning_rate: 0.0000018
sequence_per_sec: 29.9053288
2020-12-15 17:02:48,244 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 90 | Iter 720:
per_token_loss: 8.1362972 | avg_token_loss: 8.3865967 | learning_rate: 0.0000019
sequence_per_sec: 29.8735265
2020-12-15 17:07:13,568 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 95 | Iter 760:
per_token_loss: 8.0124426 | avg_token_loss: 8.3694124 | learning_rate: 0.0000021
sequence_per_sec: 29.8411641
2020-12-15 17:11:38,174 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 100 | Iter 800:
per_token_loss: 8.1410456 | avg_token_loss: 8.3521700 | learning_rate: 0.0000022
sequence_per_sec: 29.8085774
2020-12-15 17:16:03,505 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 105 | Iter 840:
per_token_loss: 7.9855108 | avg_token_loss: 8.3358879 | learning_rate: 0.0000023
sequence_per_sec: 29.7372522
2020-12-15 17:20:28,876 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 110 | Iter 880:
per_token_loss: 7.9805431 | avg_token_loss: 8.3200645 | learning_rate: 0.0000024
sequence_per_sec: 29.6680259
2020-12-15 17:24:54,112 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 115 | Iter 920:
per_token_loss: 7.9709663 | avg_token_loss: 8.3043633 | learning_rate: 0.0000025
sequence_per_sec: 29.6256661
2020-12-15 17:29:18,368 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 120 | Iter 960:
per_token_loss: 7.8385644 | avg_token_loss: 8.2883234 | learning_rate: 0.0000026
sequence_per_sec: 29.6144401
2020-12-15 17:33:43,171 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 125 | Iter 1000:
per_token_loss: 7.8986301 | avg_token_loss: 8.2734404 | learning_rate: 0.0000027
sequence_per_sec: 29.5465249
2020-12-15 17:38:22,833 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 130 | Iter 1040:
per_token_loss: 7.8574438 | avg_token_loss: 8.2576160 | learning_rate: 0.0000028
sequence_per_sec: 29.5005910
2020-12-15 17:42:45,939 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 135 | Iter 1080:
per_token_loss: 7.8484144 | avg_token_loss: 8.2416258 | learning_rate: 0.0000029
sequence_per_sec: 29.5576727
2020-12-15 17:47:09,906 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 140 | Iter 1120:
per_token_loss: 7.8947268 | avg_token_loss: 8.2262650 | learning_rate: 0.0000030
sequence_per_sec: 29.5819021
2020-12-15 17:51:33,365 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 145 | Iter 1160:
per_token_loss: 7.9661565 | avg_token_loss: 8.2116880 | learning_rate: 0.0000031
sequence_per_sec: 29.6016195
2020-12-15 17:55:57,310 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 150 | Iter 1200:
per_token_loss: 7.8537846 | avg_token_loss: 8.1985035 | learning_rate: 0.0000033
sequence_per_sec: 29.5687654
2020-12-15 18:00:21,809 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 155 | Iter 1240:
per_token_loss: 7.7590461 | avg_token_loss: 8.1856623 | learning_rate: 0.0000034
sequence_per_sec: 29.5300289
2020-12-15 18:04:47,262 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 160 | Iter 1280:
per_token_loss: 7.8727207 | avg_token_loss: 8.1740303 | learning_rate: 0.0000035
sequence_per_sec: 29.4433205
2020-12-15 18:09:13,345 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 165 | Iter 1320:
per_token_loss: 7.7211380 | avg_token_loss: 8.1616783 | learning_rate: 0.0000036
sequence_per_sec: 29.4130612
2020-12-15 18:13:37,110 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 170 | Iter 1360:
per_token_loss: 7.8505011 | avg_token_loss: 8.1492672 | learning_rate: 0.0000037
sequence_per_sec: 29.4184017
2020-12-15 18:18:07,023 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 175 | Iter 1400:
per_token_loss: 7.6416855 | avg_token_loss: 8.1375551 | learning_rate: 0.0000038
sequence_per_sec: 29.3738406
2020-12-15 18:22:35,182 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 180 | Iter 1440:
per_token_loss: 7.5637336 | avg_token_loss: 8.1258211 | learning_rate: 0.0000039
sequence_per_sec: 29.3359307
2020-12-15 18:27:04,247 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 185 | Iter 1480:
per_token_loss: 7.6566586 | avg_token_loss: 8.1134510 | learning_rate: 0.0000040
sequence_per_sec: 29.3639971
2020-12-15 18:31:33,159 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 190 | Iter 1520:
per_token_loss: 7.7162762 | avg_token_loss: 8.1028490 | learning_rate: 0.0000041
sequence_per_sec: 29.3081939
2020-12-15 18:35:57,945 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 195 | Iter 1560:
per_token_loss: 7.7450514 | avg_token_loss: 8.0924711 | learning_rate: 0.0000042
sequence_per_sec: 29.2880955
2020-12-15 18:40:21,715 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 200 | Iter 1600:
per_token_loss: 7.6374254 | avg_token_loss: 8.0813570 | learning_rate: 0.0000043
sequence_per_sec: 29.3196766
2020-12-15 18:44:45,070 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 205 | Iter 1640:
per_token_loss: 7.6665950 | avg_token_loss: 8.0706272 | learning_rate: 0.0000045
sequence_per_sec: 29.3235881
2020-12-15 18:49:09,795 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 210 | Iter 1680:
per_token_loss: 7.6826649 | avg_token_loss: 8.0609436 | learning_rate: 0.0000046
sequence_per_sec: 29.2700994
2020-12-15 18:53:36,904 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 215 | Iter 1720:
per_token_loss: 7.5332541 | avg_token_loss: 8.0508413 | learning_rate: 0.0000047
sequence_per_sec: 29.2499232
2020-12-15 18:58:02,729 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 220 | Iter 1760:
per_token_loss: 7.4305897 | avg_token_loss: 8.0413809 | learning_rate: 0.0000048
sequence_per_sec: 29.2189316
2020-12-15 19:02:28,482 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 225 | Iter 1800:
per_token_loss: 7.5136919 | avg_token_loss: 8.0316906 | learning_rate: 0.0000049
sequence_per_sec: 29.2117882
2020-12-15 19:07:00,001 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 230 | Iter 1840:
per_token_loss: 7.5271072 | avg_token_loss: 8.0228376 | learning_rate: 0.0000050
sequence_per_sec: 29.1612944
2020-12-15 19:11:23,909 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 235 | Iter 1880:
per_token_loss: 7.5600505 | avg_token_loss: 8.0132570 | learning_rate: 0.0000051
sequence_per_sec: 29.1465712
Traceback (most recent call last):
  File "/root/multiASR/egs/micArray/s5/../../../src/train.py", line 149, in <module>
    trainer.train()
  File "/root/multiASR/src/trainer.py", line 166, in train
    tr_loss = self.iter_one_epoch()
  File "/root/multiASR/src/trainer.py", line 245, in iter_one_epoch
    lst_t=self.lst_t)
  File "/root/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/root/.local/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 155, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/root/.local/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 165, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/root/.local/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 77, in parallel_apply
    thread.join()
  File "/root/anaconda3/lib/python3.7/threading.py", line 1044, in join
    self._wait_for_tstate_lock()
  File "/root/anaconda3/lib/python3.7/threading.py", line 1060, in _wait_for_tstate_lock
    elif lock.acquire(block, timeout):
  File "/root/.local/lib/python3.7/site-packages/torch/utils/data/_utils/signal_handling.py", line 66, in handler
    _error_if_any_worker_fails()
RuntimeError: DataLoader worker (pid 1346206) is killed by signal: Killed. 
Training a baseline transformer ASR system...
2020-12-18 09:36:07,123 - /root/multiASR/egs/micArray/s5/../../../src/train.py[line:126] - INFO: 
Model info:
Model(
  (splayer): SPLayer(
    (CConv): ComplexConv(
      (conv_re): Conv2d(16, 16, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))
      (conv_im): Conv2d(16, 16, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))
    )
    (LastCConv): ComplexConv(
      (conv_re): Conv2d(16, 1, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))
      (conv_im): Conv2d(16, 1, kernel_size=(3, 5), stride=(1, 1), padding=(1, 2))
    )
    (LastConv): Conv2d(2, 1, kernel_size=(1, 1), stride=(1, 1))
  )
  (encoder): Transformer(
    (sub): Conv2dSubsampleV2(
      (conv): Sequential(
        (subsample/conv0): Conv2d(1, 32, kernel_size=(3, 3), stride=(2, 1))
        (subsample/relu0): ReLU()
        (subsample/conv1): Conv2d(32, 32, kernel_size=(3, 3), stride=(2, 1))
        (subsample/relu1): ReLU()
      )
      (affine): Linear(in_features=8096, out_features=512, bias=True)
    )
    (pe): PositionalEncoding()
    (dropout): Dropout(p=0.1, inplace=False)
    (transformer_encoder): TransformerEncoder(
      (layers): ModuleList(
        (0): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (3): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (4): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
        (5): TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
        )
      )
      (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (decoder): TransformerDecoder(
    (emb): Embedding(5060, 512)
    (pe): PositionalEncoding()
    (dropout): Dropout(p=0.1, inplace=False)
    (transformer_block): TransformerDecoder(
      (layers): ModuleList(
        (0): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (1): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (2): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (3): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (4): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
        (5): TransformerDecoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (multihead_attn): MultiheadAttention(
            (out_proj): Linear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=4096, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (dropout3): Dropout(p=0.1, inplace=False)
        )
      )
    )
    (output_affine): Linear(in_features=512, out_features=5060, bias=True)
  )
)
2020-12-18 09:36:07,123 - /root/multiASR/egs/micArray/s5/../../../src/train.py[line:137] - INFO: Let's use 6 GPUs!
2020-12-18 09:36:10,879 - /root/multiASR/egs/micArray/s5/../../../src/train.py[line:148] - INFO: Start training...
2020-12-18 09:36:10,879 - /root/multiASR/src/trainer.py[line:165] - INFO: Training
/pytorch/aten/src/ATen/native/BinaryOps.cpp:81: UserWarning: Integer division of tensors using div or / is deprecated, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.
/root/.local/lib/python3.7/site-packages/torch/nn/parallel/_functions.py:61: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn('Was asked to gather along dimension 0, but all '
2020-12-18 09:44:09,372 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 5 | Iter 40:
per_token_loss: 8.6151161 | avg_token_loss: 8.6076918 | learning_rate: 0.0000001
sequence_per_sec: 16.6408532
2020-12-18 09:51:11,548 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 10 | Iter 80:
per_token_loss: 8.6094952 | avg_token_loss: 8.6057177 | learning_rate: 0.0000002
sequence_per_sec: 16.7945052
2020-12-18 09:58:20,513 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 15 | Iter 120:
per_token_loss: 8.5842800 | avg_token_loss: 8.6022959 | learning_rate: 0.0000003
sequence_per_sec: 17.1209694
2020-12-18 10:05:50,078 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 20 | Iter 160:
per_token_loss: 8.5760803 | avg_token_loss: 8.5970287 | learning_rate: 0.0000004
sequence_per_sec: 17.5060922
2020-12-18 10:13:08,789 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 25 | Iter 200:
per_token_loss: 8.5638552 | avg_token_loss: 8.5902185 | learning_rate: 0.0000005
sequence_per_sec: 17.6710395
2020-12-18 10:20:34,659 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 30 | Iter 240:
per_token_loss: 8.5207281 | avg_token_loss: 8.5831223 | learning_rate: 0.0000006
sequence_per_sec: 17.7652592
2020-12-18 10:28:07,188 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 35 | Iter 280:
per_token_loss: 8.5268183 | avg_token_loss: 8.5737915 | learning_rate: 0.0000007
sequence_per_sec: 17.6885013
2020-12-18 10:35:26,013 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 40 | Iter 320:
per_token_loss: 8.5050745 | avg_token_loss: 8.5633507 | learning_rate: 0.0000009
sequence_per_sec: 17.6046936
2020-12-18 10:42:35,789 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 45 | Iter 360:
per_token_loss: 8.4318905 | avg_token_loss: 8.5515289 | learning_rate: 0.0000010
sequence_per_sec: 17.5067100
2020-12-18 10:49:57,415 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 50 | Iter 400:
per_token_loss: 8.3461914 | avg_token_loss: 8.5366907 | learning_rate: 0.0000011
sequence_per_sec: 17.5886460
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called2020-12-18 10:57:21,492 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 55 | Iter 440:
per_token_loss: 8.3759470 | avg_token_loss: 8.5219202 | learning_rate: 0.0000012
sequence_per_sec: 17.5593359
2020-12-18 11:04:38,331 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 60 | Iter 480:
per_token_loss: 8.3119097 | avg_token_loss: 8.5068703 | learning_rate: 0.0000013
sequence_per_sec: 17.5622233
2020-12-18 11:11:48,560 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 65 | Iter 520:
per_token_loss: 8.2812510 | avg_token_loss: 8.4911699 | learning_rate: 0.0000014
sequence_per_sec: 17.5389405
2020-12-18 11:19:10,354 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 70 | Iter 560:
per_token_loss: 8.1868362 | avg_token_loss: 8.4736338 | learning_rate: 0.0000015
sequence_per_sec: 17.5559690
2020-12-18 11:26:28,429 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 75 | Iter 600:
per_token_loss: 8.1172380 | avg_token_loss: 8.4576597 | learning_rate: 0.0000016
sequence_per_sec: 17.4573377
2020-12-18 11:33:44,526 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 80 | Iter 640:
per_token_loss: 8.1679411 | avg_token_loss: 8.4399509 | learning_rate: 0.0000017
sequence_per_sec: 17.4166009
2020-12-18 11:41:04,996 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 85 | Iter 680:
per_token_loss: 8.0434513 | avg_token_loss: 8.4209766 | learning_rate: 0.0000018
sequence_per_sec: 17.4474110
2020-12-18 11:48:19,437 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 90 | Iter 720:
per_token_loss: 8.0548916 | avg_token_loss: 8.4022636 | learning_rate: 0.0000019
sequence_per_sec: 17.4675609
2020-12-18 11:55:47,862 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 95 | Iter 760:
per_token_loss: 8.0938597 | avg_token_loss: 8.3835917 | learning_rate: 0.0000021
sequence_per_sec: 17.4493180
2020-12-18 12:02:59,197 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 100 | Iter 800:
per_token_loss: 7.9901838 | avg_token_loss: 8.3650789 | learning_rate: 0.0000022
sequence_per_sec: 17.4708236
2020-12-18 12:10:02,187 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 105 | Iter 840:
per_token_loss: 8.0230551 | avg_token_loss: 8.3484325 | learning_rate: 0.0000023
sequence_per_sec: 17.4450404

WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called2020-12-18 12:17:23,453 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 110 | Iter 880:
per_token_loss: 8.0086031 | avg_token_loss: 8.3308382 | learning_rate: 0.0000024
sequence_per_sec: 17.4498217
2020-12-18 12:24:50,464 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 115 | Iter 920:
per_token_loss: 7.9502110 | avg_token_loss: 8.3131552 | learning_rate: 0.0000025
sequence_per_sec: 17.4683492
2020-12-18 12:31:57,573 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 120 | Iter 960:
per_token_loss: 7.9713721 | avg_token_loss: 8.2968740 | learning_rate: 0.0000026
sequence_per_sec: 17.4743479
2020-12-18 12:39:22,824 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 125 | Iter 1000:
per_token_loss: 7.8803902 | avg_token_loss: 8.2799740 | learning_rate: 0.0000027
sequence_per_sec: 17.5001418
2020-12-18 12:46:51,678 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 130 | Iter 1040:
per_token_loss: 7.7866421 | avg_token_loss: 8.2636728 | learning_rate: 0.0000028
sequence_per_sec: 17.5105551
2020-12-18 12:54:20,134 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 135 | Iter 1080:
per_token_loss: 7.8706350 | avg_token_loss: 8.2485657 | learning_rate: 0.0000029
sequence_per_sec: 17.5055960
2020-12-18 13:01:39,555 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 140 | Iter 1120:
per_token_loss: 7.6995792 | avg_token_loss: 8.2347240 | learning_rate: 0.0000030
sequence_per_sec: 17.4807591
2020-12-18 13:08:54,275 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 145 | Iter 1160:
per_token_loss: 7.6837535 | avg_token_loss: 8.2210073 | learning_rate: 0.0000031
sequence_per_sec: 17.4739329
2020-12-18 13:16:21,501 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 150 | Iter 1200:
per_token_loss: 7.8894291 | avg_token_loss: 8.2069483 | learning_rate: 0.0000033
sequence_per_sec: 17.4823506
2020-12-18 13:23:47,218 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 155 | Iter 1240:
per_token_loss: 7.7716455 | avg_token_loss: 8.1930561 | learning_rate: 0.0000034
sequence_per_sec: 17.4855492
2020-12-18 13:31:11,475 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 160 | Iter 1280:
per_token_loss: 7.7529941 | avg_token_loss: 8.1801786 | learning_rate: 0.0000035
sequence_per_sec: 17.4810103

WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called2020-12-18 13:38:38,532 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 165 | Iter 1320:
per_token_loss: 7.5307636 | avg_token_loss: 8.1671896 | learning_rate: 0.0000036
sequence_per_sec: 17.4779363
2020-12-18 13:46:05,448 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 170 | Iter 1360:
per_token_loss: 7.5512729 | avg_token_loss: 8.1552773 | learning_rate: 0.0000037
sequence_per_sec: 17.4687727
2020-12-18 13:53:27,377 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 175 | Iter 1400:
per_token_loss: 7.7158675 | avg_token_loss: 8.1424685 | learning_rate: 0.0000038
sequence_per_sec: 17.4863811
2020-12-18 14:00:36,039 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 180 | Iter 1440:
per_token_loss: 7.6411924 | avg_token_loss: 8.1312704 | learning_rate: 0.0000039
sequence_per_sec: 17.4836383
2020-12-18 14:07:50,691 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 185 | Iter 1480:
per_token_loss: 7.5964713 | avg_token_loss: 8.1199846 | learning_rate: 0.0000040
sequence_per_sec: 17.4819573
2020-12-18 14:15:09,258 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 190 | Iter 1520:
per_token_loss: 7.7646298 | avg_token_loss: 8.1086597 | learning_rate: 0.0000041
sequence_per_sec: 17.4823863
2020-12-18 14:22:25,096 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 195 | Iter 1560:
per_token_loss: 7.6760139 | avg_token_loss: 8.0974588 | learning_rate: 0.0000042
sequence_per_sec: 17.4953391
2020-12-18 14:29:51,138 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 200 | Iter 1600:
per_token_loss: 8.0161343 | avg_token_loss: 8.0865154 | learning_rate: 0.0000043
sequence_per_sec: 17.4961576
2020-12-18 14:37:08,843 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 205 | Iter 1640:
per_token_loss: 7.7076669 | avg_token_loss: 8.0762529 | learning_rate: 0.0000045
sequence_per_sec: 17.4966978
2020-12-18 14:44:25,103 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 210 | Iter 1680:
per_token_loss: 7.6536298 | avg_token_loss: 8.0655937 | learning_rate: 0.0000046
sequence_per_sec: 17.5044248
2020-12-18 14:51:34,438 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 215 | Iter 1720:
per_token_loss: 7.6722698 | avg_token_loss: 8.0556412 | learning_rate: 0.0000047
sequence_per_sec: 17.4998160

WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called2020-12-18 14:58:58,974 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 220 | Iter 1760:
per_token_loss: 7.5859571 | avg_token_loss: 8.0451813 | learning_rate: 0.0000048
sequence_per_sec: 17.5095522
2020-12-18 15:06:18,076 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 225 | Iter 1800:
per_token_loss: 7.6175733 | avg_token_loss: 8.0355663 | learning_rate: 0.0000049
sequence_per_sec: 17.5076018
2020-12-18 15:13:40,962 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 230 | Iter 1840:
per_token_loss: 7.6219563 | avg_token_loss: 8.0257912 | learning_rate: 0.0000050
sequence_per_sec: 17.5057397
2020-12-18 15:21:06,942 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 235 | Iter 1880:
per_token_loss: 7.5521526 | avg_token_loss: 8.0159197 | learning_rate: 0.0000051
sequence_per_sec: 17.5097294
2020-12-18 15:28:35,030 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 240 | Iter 1920:
per_token_loss: 7.5360136 | avg_token_loss: 8.0060110 | learning_rate: 0.0000052
sequence_per_sec: 17.5180644
2020-12-18 15:35:58,265 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 245 | Iter 1960:
per_token_loss: 7.4934564 | avg_token_loss: 7.9964590 | learning_rate: 0.0000053
sequence_per_sec: 17.5297175
2020-12-18 15:43:17,285 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 250 | Iter 2000:
per_token_loss: 7.4120455 | avg_token_loss: 7.9876056 | learning_rate: 0.0000054
sequence_per_sec: 17.5189199
2020-12-18 15:50:37,690 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 255 | Iter 2040:
per_token_loss: 7.5261440 | avg_token_loss: 7.9783258 | learning_rate: 0.0000055
sequence_per_sec: 17.5149308
2020-12-18 15:57:56,251 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 260 | Iter 2080:
per_token_loss: 7.6296763 | avg_token_loss: 7.9698367 | learning_rate: 0.0000057
sequence_per_sec: 17.4975854
2020-12-18 16:05:03,275 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 265 | Iter 2120:
per_token_loss: 7.4310026 | avg_token_loss: 7.9610605 | learning_rate: 0.0000058
sequence_per_sec: 17.5029010

WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called2020-12-18 16:12:18,590 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 270 | Iter 2160:
per_token_loss: 7.5680137 | avg_token_loss: 7.9521084 | learning_rate: 0.0000059
sequence_per_sec: 17.5047132
2020-12-18 16:19:37,701 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 275 | Iter 2200:
per_token_loss: 7.4448700 | avg_token_loss: 7.9433813 | learning_rate: 0.0000060
sequence_per_sec: 17.5044240
2020-12-18 16:26:55,344 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 280 | Iter 2240:
per_token_loss: 7.4977160 | avg_token_loss: 7.9340386 | learning_rate: 0.0000061
sequence_per_sec: 17.5166659
2020-12-18 16:34:04,337 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 285 | Iter 2280:
per_token_loss: 7.4638867 | avg_token_loss: 7.9260139 | learning_rate: 0.0000062
sequence_per_sec: 17.4988476
2020-12-18 16:41:22,249 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 290 | Iter 2320:
per_token_loss: 7.3277206 | avg_token_loss: 7.9172873 | learning_rate: 0.0000063
sequence_per_sec: 17.5030525
2020-12-18 16:48:47,852 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 295 | Iter 2360:
per_token_loss: 7.3986177 | avg_token_loss: 7.9079685 | learning_rate: 0.0000064
sequence_per_sec: 17.5192511
2020-12-18 16:56:13,760 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 300 | Iter 2400:
per_token_loss: 7.3406987 | avg_token_loss: 7.8991480 | learning_rate: 0.0000065
sequence_per_sec: 17.5192545
2020-12-18 17:03:31,663 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 305 | Iter 2440:
per_token_loss: 7.4832163 | avg_token_loss: 7.8909221 | learning_rate: 0.0000066
sequence_per_sec: 17.5106139
2020-12-18 17:10:46,981 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 310 | Iter 2480:
per_token_loss: 7.5524216 | avg_token_loss: 7.8826771 | learning_rate: 0.0000067
sequence_per_sec: 17.5034820
2020-12-18 17:18:06,711 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 315 | Iter 2520:
per_token_loss: 7.1191239 | avg_token_loss: 7.8740182 | learning_rate: 0.0000069
sequence_per_sec: 17.4983331
2020-12-18 17:25:24,329 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 320 | Iter 2560:
per_token_loss: 7.3857946 | avg_token_loss: 7.8654208 | learning_rate: 0.0000070
sequence_per_sec: 17.4920961

WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called2020-12-18 17:32:39,171 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 325 | Iter 2600:
per_token_loss: 7.2905259 | avg_token_loss: 7.8567567 | learning_rate: 0.0000071
sequence_per_sec: 17.4824959
2020-12-18 17:39:47,705 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 330 | Iter 2640:
per_token_loss: 7.2650414 | avg_token_loss: 7.8479261 | learning_rate: 0.0000072
sequence_per_sec: 17.4885593
2020-12-18 17:46:59,610 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 335 | Iter 2680:
per_token_loss: 7.2153053 | avg_token_loss: 7.8393917 | learning_rate: 0.0000073
sequence_per_sec: 17.4765635
2020-12-18 17:54:23,117 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 340 | Iter 2720:
per_token_loss: 7.1434817 | avg_token_loss: 7.8306713 | learning_rate: 0.0000074
sequence_per_sec: 17.4713819
2020-12-18 18:01:52,070 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 345 | Iter 2760:
per_token_loss: 7.2883339 | avg_token_loss: 7.8216286 | learning_rate: 0.0000075
sequence_per_sec: 17.4681538
2020-12-18 18:09:16,510 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 350 | Iter 2800:
per_token_loss: 7.1821647 | avg_token_loss: 7.8123817 | learning_rate: 0.0000076
sequence_per_sec: 17.4767485
2020-12-18 18:16:29,793 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 355 | Iter 2840:
per_token_loss: 7.1061893 | avg_token_loss: 7.8037848 | learning_rate: 0.0000077
sequence_per_sec: 17.4761549
2020-12-18 18:23:49,582 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 360 | Iter 2880:
per_token_loss: 7.0995355 | avg_token_loss: 7.7947025 | learning_rate: 0.0000078
sequence_per_sec: 17.4790049
2020-12-18 18:31:06,539 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 365 | Iter 2920:
per_token_loss: 7.0812864 | avg_token_loss: 7.7857170 | learning_rate: 0.0000079
sequence_per_sec: 17.4808899
2020-12-18 18:38:23,198 - /root/multiASR/src/trainer.py[line:301] - INFO: Progress:
Epoch 1 | Step 370 | Iter 2960:
per_token_loss: 7.0185194 | avg_token_loss: 7.7773046 | learning_rate: 0.0000081
sequence_per_sec: 17.4644408

WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
WaveCollate called
Traceback (most recent call last):
  File "/root/multiASR/egs/micArray/s5/../../../src/train.py", line 149, in <module>
    trainer.train()
  File "/root/multiASR/src/trainer.py", line 166, in train
    tr_loss = self.iter_one_epoch()
  File "/root/multiASR/src/trainer.py", line 245, in iter_one_epoch
    lst_t=self.lst_t)
  File "/root/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/root/.local/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 155, in forward
    outputs = self.parallel_apply(replicas, inputs, kwargs)
  File "/root/.local/lib/python3.7/site-packages/torch/nn/parallel/data_parallel.py", line 165, in parallel_apply
    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])
  File "/root/.local/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 85, in parallel_apply
    output.reraise()
  File "/root/.local/lib/python3.7/site-packages/torch/_utils.py", line 395, in reraise
    raise self.exc_type(msg)
RuntimeError: Caught RuntimeError in replica 0 on device 0.
Original Traceback (most recent call last):
  File "/root/.local/lib/python3.7/site-packages/torch/nn/parallel/parallel_apply.py", line 60, in _worker
    output = module(*input, **kwargs)
  File "/root/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/root/multiASR/src/models.py", line 54, in forward
    return_atten=True)
  File "/root/multiASR/src/models.py", line 115, in get_logits
    sp_outputs, sp_output_lengths = self.splayer(batch_wave, lengths)
  File "/root/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/root/multiASR/src/sp_layers.py", line 148, in forward
    padded_features = self.LastConv(padded_features).squeeze()
  File "/root/.local/lib/python3.7/site-packages/torch/nn/modules/module.py", line 550, in __call__
    result = self.forward(*input, **kwargs)
  File "/root/.local/lib/python3.7/site-packages/torch/nn/modules/conv.py", line 349, in forward
    return self._conv_forward(input, self.weight)
  File "/root/.local/lib/python3.7/site-packages/torch/nn/modules/conv.py", line 346, in _conv_forward
    self.padding, self.dilation, self.groups)
RuntimeError: Expected 4-dimensional input for 4-dimensional weight [1, 2, 1, 1], but got 3-dimensional input of size [2, 4201, 257] instead

